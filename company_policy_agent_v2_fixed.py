#!/usr/bin/env python3
# -*- coding: utf-8 -*-
import streamlit as st
import pandas as pd
import os, base64, glob, html, datetime, hashlib, uuid, csv
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain.vectorstores import Chroma
from langchain_community.embeddings import OllamaEmbeddings
from langchain.embeddings import OpenAIEmbeddings
from langchain.schema import Document
from langchain.document_loaders import PyPDFLoader, TextLoader

# =========================
# ê¸°ë³¸ ì„¤ì •
# =========================
st.set_page_config(page_title="ì‚¬ë‚´ ì •ì±… RAG ì—ì´ì „íŠ¸", layout="wide")
DATA_DIR = "./rag_data"
SAMPLE_DOCS_DIR = "./sample_docs"
LOG_FILE = "./qa_logs.csv"
os.makedirs(DATA_DIR, exist_ok=True)
os.makedirs(SAMPLE_DOCS_DIR, exist_ok=True)
CSV_CACHE = {}

# í˜•íƒœì†Œ ë¶„ì„ê¸°
try:
    from konlpy.tag import Okt
    okt = Okt()
except Exception:
    okt = None
    st.warning("âš ï¸ í˜•íƒœì†Œ ë¶„ì„ê¸°(KoNLPy)ë¥¼ ì‚¬ìš©í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤. í‚¤ì›Œë“œ ì¶”ì¶œì€ ë¹„í™œì„±í™”ë©ë‹ˆë‹¤.")

# ê´€ê³„ ë§¤í•‘
relation_map = {
    "ì¥ì¸": "ë°°ìš°ì ë¶€ëª¨", "ì¥ëª¨": "ë°°ìš°ì ë¶€ëª¨", "ì‹œì–´ë¨¸ë‹ˆ": "ë°°ìš°ì ë¶€ëª¨", "ì‹œì•„ë²„ì§€": "ë°°ìš°ì ë¶€ëª¨",
    "ì‹œë¶€ëª¨": "ë°°ìš°ì ë¶€ëª¨", "ì‹œë¶€ëª¨ë‹˜": "ë°°ìš°ì ë¶€ëª¨",
    "ì¹œì •ì–´ë¨¸ë‹ˆ": "ë°°ìš°ì ë¶€ëª¨", "ì¹œì •ì•„ë²„ì§€": "ë°°ìš°ì ë¶€ëª¨",
    "ì•„ë²„ì§€": "ë¶€ëª¨", "ì–´ë¨¸ë‹ˆ": "ë¶€ëª¨", "ë¶€ëª¨ë‹˜": "ë¶€ëª¨",
    "ë°°ìš°ì": "ë°°ìš°ì", "ë‚¨í¸": "ë°°ìš°ì", "ì•„ë‚´": "ë°°ìš°ì",
    "ìë…€": "ìë…€", "ì•„ë“¤": "ìë…€", "ë”¸": "ìë…€"
}
def extract_relation(text):
    for k, v in relation_map.items():
        if k in text:
            return v
    return None

# =========================
# ì„¸ì…˜ ìƒíƒœ
# =========================
if "llm_choice" not in st.session_state:
    st.session_state.llm_choice = "Ollama (ë¡œì»¬)"
if "openai_key" not in st.session_state:
    st.session_state.openai_key = None

# =========================
# í—¬í¼ í•¨ìˆ˜
# =========================
def get_db_dir(use_ollama: bool):
    db_dir = f"./vector_db_{'ollama' if use_ollama else 'openai'}"
    os.makedirs(db_dir, exist_ok=True)
    return db_dir

def make_doc_hash(doc: Document):
    idx = str(doc.metadata.get('chunk_index') if doc.metadata.get('chunk_index') is not None else uuid.uuid4())
    key = f"{doc.metadata.get('ID','')}_{idx}_{doc.metadata.get('is_augmented',False)}"
    return hashlib.md5(key.encode()).hexdigest()

# **ë³µì›ëœ augment_question**
def augment_question(question, base_meta):
    if not question or not question.strip():
        return []
    synonyms = {"ê²½ì¡°ê¸ˆ": ["ì§€ì›ê¸ˆ", "ê²½ì¡°ì‚¬ë¹„"], "íœ´ê°€": ["íœ´ì¼", "íœ´ë¬´"]}
    aug_questions = [question]
    for key, values in synonyms.items():
        for v in values:
            if key in question:
                aug_questions.append(question.replace(key, v))
    if "?" in question:
        aug_questions.append(question.replace("?", " ì•Œë ¤ì£¼ì„¸ìš”."))
    docs = []
    for q in set(aug_questions):
        meta = base_meta.copy()
        meta["is_augmented"] = True
        meta["chunk_index"] = str(uuid.uuid4())
        meta["parent_id"] = str(base_meta.get("ID"))
        new_doc = Document(page_content=q, metadata=meta)
        new_doc.metadata['doc_hash'] = make_doc_hash(new_doc)
        docs.append(new_doc)
    return docs

def load_and_split(file_path):
    ext = os.path.splitext(file_path)[1].lower()
    docs = []
    try:
        if ext == ".csv":
            df = pd.read_csv(file_path, encoding="utf-8-sig")
            if 'ID' not in df.columns or 'ë‹µë³€' not in df.columns:
                st.error(f"{os.path.basename(file_path)}: 'ID'ì™€ 'ë‹µë³€' ì»¬ëŸ¼ì´ í•„ìš”í•©ë‹ˆë‹¤.")
                return []
            CSV_CACHE[file_path] = df  # ì—…ë¡œë“œ ì‹œ ìºì‹±
            for _, row in df.iterrows():
                content = row.get("ë‹µë³€", "")
                metadata = {
                    "ì§ˆë¬¸": row.get("ì§ˆë¬¸", ""),
                    "ID": str(row.get("ID", "")),
                    "ì¡°í•­": row.get("ì¡°í•­", ""),
                    "ì¹´í…Œê³ ë¦¬": row.get("ì¹´í…Œê³ ë¦¬", ""),
                    "ëŒ€ìƒ": str(row.get("ëŒ€ìƒ", "")) if 'ëŒ€ìƒ' in df.columns else "",
                    "is_augmented": False,
                }
                docs.append(Document(page_content=str(content), metadata=metadata))
        elif ext == ".pdf":
            loader = PyPDFLoader(file_path)
            docs = loader.load()
        else:
            loader = TextLoader(file_path, encoding="utf-8")
            docs = loader.load()
    except Exception as e:
        st.error(f"íŒŒì¼ ë¡œë“œ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        return []
    splitter = RecursiveCharacterTextSplitter(chunk_size=500, chunk_overlap=50)
    chunks = splitter.split_documents(docs)
    for idx, chunk in enumerate(chunks):
        chunk.metadata['chunk_index'] = idx
        chunk.metadata['doc_hash'] = make_doc_hash(chunk)
    return chunks

def build_vector_db(docs, embedding_model, persist_dir):
    if os.listdir(persist_dir):
        vectordb = Chroma(persist_directory=persist_dir, embedding_function=embedding_model)
        existing_data = vectordb.get() or {}
        metas = existing_data.get('metadatas') or []
        existing_hashes = {m.get('doc_hash') for m in metas if isinstance(m, dict)}
        new_docs = [d for d in docs if d.metadata.get('doc_hash') not in existing_hashes]
        if new_docs:
            vectordb.add_documents(new_docs)
    else:
        Chroma.from_documents(docs, embedding_model, persist_directory=persist_dir)

def get_embedding(use_ollama=True, openai_api_key=None):
    return OllamaEmbeddings(model="llama3") if use_ollama else OpenAIEmbeddings(openai_api_key=openai_api_key)

def log_qa(question, answer, score, faq_id, is_augmented):
    now = datetime.datetime.now().strftime("%Y-%m-%d %H:%M:%S")
    log_exists = os.path.exists(LOG_FILE)
    with open(LOG_FILE, "a", encoding="utf-8-sig", newline='') as f:
        writer = csv.writer(f)
        if not log_exists:
            writer.writerow(["timestamp","question","answer","score","faq_id","is_augmented"])
        writer.writerow([now, question, answer, score, faq_id, is_augmented])

def make_download_button(file_path):
    if not file_path: return ""
    with open(file_path, "rb") as f: data = f.read()
    b64 = base64.b64encode(data).decode()
    return f"""<a href="data:application/octet-stream;base64,{b64}" download="{os.path.basename(file_path)}">
        <button style="background-color:#007bff;color:white;padding:6px 14px;border:none;border-radius:5px;font-weight:500;cursor:pointer;">ê²½ì¡°ì‚¬ ì‹ ì²­ì„œ ìƒ˜í”Œ ë‹¤ìš´ë°›ê¸°</button></a>"""

def make_link_button():
    return f"""<a href="https://www.notion.so/23838859e65c8049bb8ae5dc6540889b?source=copy_link" target="_blank">
        <button style="background-color:#28a745;color:white;padding:6px 14px;border:none;border-radius:5px;font-weight:500;cursor:pointer;">ê²½ì¡°ì‚¬ ì‹ ì²­í•˜ê¸°</button></a>"""

def extract_keywords(text):
    if okt:
        nouns = okt.nouns(text)
        weighted = {n: len(n) * nouns.count(n) for n in set(nouns)}
        return sorted(weighted.items(), key=lambda x: x[1], reverse=True)[:5]
    return []

def render_answer(answer, faq_id, score, chunk_idx, keywords):
    chunk_display = "ì¦ê°•" if chunk_idx and isinstance(chunk_idx, str) and len(str(chunk_idx)) > 5 else chunk_idx
    base_id = faq_id.split('-')[0] if faq_id else ''
    sample_files = glob.glob(os.path.join(SAMPLE_DOCS_DIR, f"{base_id}*.docx"))
    sample_file_path = sample_files[0] if sample_files else None
    safe_answer = html.escape(answer).replace("\n", "<br>")
    safe_answer = safe_answer.replace(html.escape("[ê²½ì¡°ì‚¬ ì‹ ì²­ì„œ ìƒ˜í”Œ ë‹¤ìš´ë°›ê¸°]"), f"""<div style="display:inline-block; margin-right:8px;">{make_download_button(sample_file_path)}</div>""")
    safe_answer = safe_answer.replace(html.escape("[ê²½ì¡°ì‚¬ ì‹ ì²­í•˜ê¸°]"), f"""<div style="display:inline-block; margin-right:8px;">{make_link_button()}</div>""")
    st.markdown(
        f"""
        <div style="background-color:#f0f9ff;padding:16px;border-radius:8px;line-height:1.5; font-size:15px;">
            {safe_answer}
        </div>
        <div style="background-color:#f8f9fa;padding:8px;border-radius:5px;margin-top:8px; font-size:14px;">
            <b>ìœ ì‚¬ë„ ì ìˆ˜:</b> {round(score,3)} |
            <b>ì²­í‚¹ ìœ„ì¹˜:</b> {chunk_display} |
            <b>í‚¤ì›Œë“œ:</b> {', '.join([f"{k}({v})" for k,v in keywords]) if keywords else 'ì—†ìŒ'}
        </div>""",
        unsafe_allow_html=True
    )

# =========================
# UI
# =========================
st.title("ğŸ¢ ì‚¬ë‚´ ì •ì±… RAG ì—ì´ì „íŠ¸ (Ollama + OpenAI)")
tab1, tab2, tab3 = st.tabs(["ğŸ“‚ ë°ì´í„° ì—…ë¡œë“œ", "ğŸ’¬ ì§ˆì˜ì‘ë‹µ", "ğŸ“ˆ ì„±ëŠ¥ í‰ê°€"])

with tab1:
    st.subheader("ë¬¸ì„œ ì—…ë¡œë“œ & ì„ë² ë”©")
    uploaded_files = st.file_uploader("ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ì„¸ìš” (PDF, CSV, TXT)", type=["pdf","csv","txt"], accept_multiple_files=True)
    st.session_state.llm_choice = st.radio("LLM ì„ íƒ", ["Ollama (ë¡œì»¬)", "OpenAI API"], index=0)
    st.session_state.openai_key = st.text_input("OpenAI API í‚¤ (OpenAI ì„ íƒ ì‹œ)", type="password") if st.session_state.llm_choice == "OpenAI API" else None
    if st.button("ğŸš€ ì—…ë¡œë“œ & ì„ë² ë”© ì‹¤í–‰"):
        if st.session_state.llm_choice == "OpenAI API" and not st.session_state.openai_key:
            st.error("âš ï¸ OpenAI API í‚¤ë¥¼ ì…ë ¥í•´ì£¼ì„¸ìš”.")
            st.stop()
        if not uploaded_files:
            st.warning("ì—…ë¡œë“œí•  ë¬¸ì„œë¥¼ ì„ íƒí•˜ì„¸ìš”!")
        else:
            total_docs, total_aug = 0, 0
            with st.spinner("ë¬¸ì„œ ì²˜ë¦¬ ë° ì¦ê°• ì¤‘..."):
                db_dir = get_db_dir(st.session_state.llm_choice == "Ollama (ë¡œì»¬)")
                embedding_model = get_embedding(st.session_state.llm_choice == "Ollama (ë¡œì»¬)", st.session_state.openai_key)
                for file in uploaded_files:
                    file_path = os.path.join(DATA_DIR, file.name)
                    with open(file_path, "wb") as f: f.write(file.read())
                    docs = load_and_split(file_path)
                    if not docs: continue
                    aug_docs = []
                    for d in docs:
                        if d.metadata.get("ì§ˆë¬¸","").strip():
                            aug_docs += augment_question(d.metadata.get("ì§ˆë¬¸"), d.metadata)
                    build_vector_db(docs + aug_docs, embedding_model, db_dir)
                    total_docs += len(docs)
                    total_aug += len(aug_docs)
                st.success(f"âœ… ë¬¸ì„œ ì—…ë¡œë“œ ë° ì¦ê°• ì™„ë£Œ! (ì›ë³¸ {total_docs}, ì¦ê°• í›„ {total_docs+total_aug})")

with tab2:
    st.subheader("ì§ˆë¬¸í•˜ê¸° (RAG ê¸°ë°˜)")
    query = st.text_input("ì§ˆë¬¸ì„ ì…ë ¥í•˜ì„¸ìš”", placeholder="ì˜ˆ: ì¥ì¸ì–´ë¥¸ì´ ëŒì•„ê°€ì…¨ì–´ìš”. ê²½ì¡°ê¸ˆì€?")
    if st.button("ğŸ” ê²€ìƒ‰ & ë‹µë³€"):
        # ìºì‹œ ë¹„ì–´ìˆìœ¼ë©´ CSV ë‹¤ì‹œ ë¡œë“œ
        if not CSV_CACHE:
            for file in os.listdir(DATA_DIR):
                if file.endswith(".csv"):
                    df = pd.read_csv(os.path.join(DATA_DIR, file), encoding="utf-8-sig")
                    CSV_CACHE[file] = df

        db_dir = get_db_dir(st.session_state.llm_choice == "Ollama (ë¡œì»¬)")
        if not os.listdir(db_dir):
            st.error("ë°ì´í„°ë² ì´ìŠ¤ê°€ ë¹„ì–´ ìˆìŠµë‹ˆë‹¤. ë¨¼ì € ë¬¸ì„œë¥¼ ì—…ë¡œë“œí•˜ê³  ì„ë² ë”©í•˜ì„¸ìš”.")
            st.stop()
        with st.spinner("ê²€ìƒ‰ ì¤‘..."):
            relation = extract_relation(query)
            found_answer, target_id = None, None

            # 1ë‹¨ê³„: ê´€ê³„ ë§¤ì¹­
            if relation:
                for df in CSV_CACHE.values():
                    match = df[df['ëŒ€ìƒ'] == relation]
                    if not match.empty:
                        found_answer = str(match.iloc[0]['ë‹µë³€'])
                        target_id = str(match.iloc[0]['ID'])
                        break

            # 2ë‹¨ê³„: ë²¡í„° ê²€ìƒ‰
            if not found_answer:
                embedding_model = get_embedding(st.session_state.llm_choice == "Ollama (ë¡œì»¬)", st.session_state.openai_key)
                vectordb = Chroma(persist_directory=db_dir, embedding_function=embedding_model)
                docs = vectordb.similarity_search_with_score(query, k=1)
                if docs:
                    top_doc, score = docs[0]
                    target_id = str(top_doc.metadata.get("parent_id") if top_doc.metadata.get("is_augmented") else top_doc.metadata.get("ID"))
                    for df in CSV_CACHE.values():
                        match = df[df['ID'].astype(str) == target_id]
                        if not match.empty:
                            found_answer = str(match.iloc[0]['ë‹µë³€'])
                            break
                    if not found_answer:
                        found_answer = top_doc.page_content.strip() or "âš ï¸ ê´€ë ¨ ë‹µë³€ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤."

            if found_answer:
                keywords = extract_keywords(query)
                render_answer(found_answer, target_id or "-", 1.0, "-", keywords)
                log_qa(query, found_answer, 1.0, target_id or "-", False)
            else:
                st.warning("ê´€ë ¨ FAQë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.subheader("ì„±ëŠ¥ í‰ê°€")
    if os.path.exists(LOG_FILE):
        df = pd.read_csv(LOG_FILE)
        if not df.empty:
            df['score'] = pd.to_numeric(df['score'], errors='coerce')
            df['is_augmented'] = df['is_augmented'].map({'True': True, 'False': False}).fillna(False)
            avg_score = df['score'].mean()
            match_rate = (df['score'] >= 0.8).mean() * 100
            st.metric("ì „ì²´ í‰ê·  ìœ ì‚¬ë„", f"{avg_score:.3f}")
            st.metric("ì „ì²´ ë§¤ì¹­ë¥ (0.8 ì´ìƒ)", f"{match_rate:.1f}%")
            if 'is_augmented' in df.columns:
                before = df[df['is_augmented']==False]['score']
                after = df[df['is_augmented']==True]['score']
                comp = pd.DataFrame({
                    'ìœ í˜•': ['ì›ë³¸','ì¦ê°•'],
                    'í‰ê·  ìœ ì‚¬ë„': [before.mean() if not before.empty else 0, after.mean() if not after.empty else 0]
                })
                if not comp.empty:
                    st.bar_chart(comp.set_index('ìœ í˜•'))
            if not df['score'].dropna().empty:
                st.line_chart(df['score'].dropna())
            st.dataframe(df.tail(20))
            csv_data = df.to_csv(index=False).encode('utf-8-sig')
            st.download_button("CSV ë‹¤ìš´ë¡œë“œ", csv_data, "qa_logs.csv")
    else:
        st.info("ì•„ì§ ë¡œê·¸ê°€ ì—†ìŠµë‹ˆë‹¤. ì§ˆë¬¸ì„ ì…ë ¥í•´ë³´ì„¸ìš”.")
